{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW, lr_scheduler\n",
    "from torch.distributions import Uniform\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchaudio.transforms import Spectrogram, MelSpectrogram\n",
    "from torchaudio.transforms import TimeStretch, AmplitudeToDB, ComplexNorm, Resample\n",
    "from torchaudio.transforms import FrequencyMasking, TimeMasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIRD_CODE = {\n",
    "    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n",
    "    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n",
    "    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n",
    "    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n",
    "    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n",
    "    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n",
    "    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n",
    "    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n",
    "    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n",
    "    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n",
    "    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n",
    "    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n",
    "    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n",
    "    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n",
    "    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n",
    "    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n",
    "    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n",
    "    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n",
    "    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n",
    "    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n",
    "    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n",
    "    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n",
    "    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n",
    "    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n",
    "    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n",
    "    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n",
    "    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n",
    "    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n",
    "    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n",
    "    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n",
    "    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n",
    "    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n",
    "    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n",
    "    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n",
    "    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n",
    "    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n",
    "    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n",
    "    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n",
    "    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n",
    "    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n",
    "    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n",
    "    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n",
    "    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n",
    "    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n",
    "    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n",
    "    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n",
    "    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n",
    "    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n",
    "    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n",
    "    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n",
    "    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n",
    "    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n",
    "    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n",
    "}\n",
    "\n",
    "INV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTrainDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 sound_dir, \n",
    "                 audio_sec=5,\n",
    "                 sample_rate=32000,\n",
    "                 max_perc=0.4\n",
    "                ):\n",
    "        \n",
    "        self.train_df = df\n",
    "        self.sound_dir = sound_dir\n",
    "        self.audio_sec = audio_sec\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_lenght = sample_rate * audio_sec\n",
    "        self.max_perc = max_perc\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        sound_info = self.train_df[ix]\n",
    "        \n",
    "        waveform = torch.load(sound_info[0])\n",
    "        target = torch.zeros([264], dtype=torch.float32)\n",
    "        target[sound_info[1].item()] = 1\n",
    "        \n",
    "        waveform = self.norm_5sec(waveform)\n",
    "        \n",
    "        return waveform, target, sound_info[1]    \n",
    "    \n",
    "    def norm_5sec(self, waveform):\n",
    "        input_audio_lenght = waveform.size(1)\n",
    "        \n",
    "        if input_audio_lenght > self.target_lenght:\n",
    "            dist = torch.randint(0, input_audio_lenght-self.target_lenght, (1,)).item()\n",
    "            waveform = waveform[:, dist:dist + self.target_lenght]\n",
    "        else:\n",
    "            waveform = torch.cat([waveform, torch.zeros([1, self.target_lenght - input_audio_lenght])], dim=1)\n",
    "        \n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RondomStretchMelSpectrogram(nn.Module):\n",
    "    def __init__(self, sample_rate, n_fft, top_db, max_perc):\n",
    "        super().__init__()\n",
    "        self.time_stretch = TimeStretch(hop_length=None, n_freq=n_fft//2+1)\n",
    "        self.stft = Spectrogram(n_fft=n_fft, power=None)\n",
    "        self.com_norm = ComplexNorm(power=2.)\n",
    "        self.fm = FrequencyMasking(100)\n",
    "        self.tm = TimeMasking(100)\n",
    "        self.mel_specgram = MelSpectrogram(sample_rate, n_fft=n_fft, f_max=8000)\n",
    "        self.AtoDB= AmplitudeToDB(top_db=top_db)\n",
    "        self.max_perc = max_perc\n",
    "        self.sample_rate = sample_rate\n",
    "        self.resamples = [\n",
    "                Resample(sample_rate, sample_rate*0.6),\n",
    "                Resample(sample_rate, sample_rate*0.7),\n",
    "                Resample(sample_rate, sample_rate*0.8),\n",
    "                Resample(sample_rate, sample_rate*0.9),\n",
    "                Resample(sample_rate, sample_rate*1),\n",
    "                Resample(sample_rate, sample_rate*1.1),\n",
    "                Resample(sample_rate, sample_rate*1.2),\n",
    "                Resample(sample_rate, sample_rate*1.3),\n",
    "                Resample(sample_rate, sample_rate*1.4)\n",
    "            ]\n",
    "    \n",
    "    def forward(self, x, train):\n",
    "        x = random.choice(self.resamples)(x)\n",
    "        \n",
    "        x = self.stft(x)\n",
    "\n",
    "        if train:\n",
    "            dist = Uniform(1.-self.max_perc, 1+self.max_perc)\n",
    "            x = self.time_stretch(x, dist.sample().item())\n",
    "            x = self.com_norm(x)\n",
    "            x = self.fm(x, 0)\n",
    "            x = self.tm(x, 0)\n",
    "        else:\n",
    "            x = self.com_norm(x)\n",
    "        \n",
    "        x = self.mel_specgram.mel_scale(x)\n",
    "        x = self.AtoDB(x)\n",
    "        \n",
    "        size = torch.tensor(x.size())\n",
    "        \n",
    "        if size[3] > 157:\n",
    "            x = x[:,:,:,0:157]\n",
    "        else:\n",
    "            x = torch.cat([x, torch.cuda.FloatTensor(size[0], size[1], size[2], 157 - size[3]).fill_(0)], dim=3)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(predict):\n",
    "    return [np.argwhere(predict[i] == predict[i].max())[0].item() for i in range(len(predict))]\n",
    "\n",
    "def get_F1_score(y_true, y_pred, average):\n",
    "    return f1_score(y_true, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_concat_pool2d(x, sz=(1,1)):\n",
    "    out1 = F.adaptive_avg_pool2d(x, sz).view(x.size(0), -1)\n",
    "    out2 = F.adaptive_max_pool2d(x, sz).view(x.size(0), -1)\n",
    "    return torch.cat([out1, out2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pool=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = kernel_size // 2\n",
    "        self.pool = pool\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels + in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x): # x.shape = [batch_size, in_channels, a, b]\n",
    "        x1 = self.conv1(x)\n",
    "        x = self.conv2(torch.cat([x, x1],1))\n",
    "        if(self.pool): x = F.avg_pool2d(x, 2)\n",
    "        return x   # x.shape = [batch_size, out_channels, a//2, b//2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_M3(nn.Module):\n",
    "    def __init__(self, num_classes=264):\n",
    "        super().__init__()\n",
    "        self.mel = RondomStretchMelSpectrogram(sample_rate=32_000, n_fft=2**11, top_db=80, max_perc=0.4)\n",
    "        self.conv1 = ConvBlock(1,64)\n",
    "        self.conv2 = ConvBlock(64,128)\n",
    "        self.conv3 = ConvBlock(128,256)\n",
    "        self.conv4 = ConvBlock(256,512)\n",
    "        self.conv5 = ConvBlock(512,1024,pool=False)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(3840),\n",
    "            nn.Linear(3840, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, train): # batch_size, 3, a, b\n",
    "        x = self.mel(x, train)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        #pyramid pooling\n",
    "        x = torch.cat([adaptive_concat_pool2d(x2), adaptive_concat_pool2d(x3),\n",
    "                       adaptive_concat_pool2d(x4),adaptive_concat_pool2d(x5)], 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, train_dataloader, test_dataloader, lr, betas, weight_decay, log_freq, with_cuda, model=None):\n",
    "        \n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda\" if cuda_condition else \"cpu\")\n",
    "        print(\"Use:\", \"cuda:0\" if cuda_condition else \"cpu\")\n",
    "        \n",
    "        self.model = Classifier_M3().to(self.device)\n",
    "        self.optim = AdamW(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.scheduler = lr_scheduler.CosineAnnealingLR(self.optim, 5)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        if model != None:            \n",
    "            checkpoint = torch.load(model)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.epoch = checkpoint['epoch']\n",
    "            self.criterion = checkpoint['loss']\n",
    "\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        print(\"Using %d GPUS for Converter\" % torch.cuda.device_count())\n",
    "        \n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "        \n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "        \n",
    "        self.test_loss = []\n",
    "        self.train_loss = []\n",
    "        self.train_f1_score = []\n",
    "        self.test_f1_score = []\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        :param epoch: 現在のepoch\n",
    "        :param data_loader: torch.utils.data.DataLoader\n",
    "        :param train: trainかtestかのbool値\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        data_iter = tqdm(enumerate(data_loader), desc=\"EP_%s:%d\" % (str_code, epoch), total=len(data_loader), bar_format=\"{l_bar}{r_bar}\")\n",
    "        \n",
    "        total_element = 0\n",
    "        loss_store = 0.0\n",
    "        f1_score_store = 0.0\n",
    "        total_correct = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            specgram = data[0].to(self.device)\n",
    "            label = data[2].to(self.device)\n",
    "            one_hot_label = data[1].to(self.device)\n",
    "            predict_label = self.model(specgram, train)\n",
    "\n",
    "            # \n",
    "            predict_f1_score = get_F1_score(\n",
    "                label.cpu().detach().numpy(),\n",
    "                convert_label(predict_label.cpu().detach().numpy()),\n",
    "                average='micro'\n",
    "            )\n",
    "            \n",
    "            loss = self.criterion(predict_label, one_hot_label)\n",
    "\n",
    "            # \n",
    "            if train:\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            loss_store += loss.item()\n",
    "            f1_score_store += predict_f1_score\n",
    "            self.avg_loss = loss_store / (i + 1)\n",
    "            self.avg_f1_score = f1_score_store / (i + 1)\n",
    "        \n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": round(self.avg_loss, 5),\n",
    "                \"loss\": round(loss.item(), 5),\n",
    "                \"avg_f1_score\": round(self.avg_f1_score, 5)\n",
    "            }\n",
    "\n",
    "        data_iter.write(str(post_fix))\n",
    "        self.train_loss.append(self.avg_loss) if train else self.test_loss.append(self.avg_loss)\n",
    "        self.train_f1_score.append(self.avg_f1_score) if train else self.test_f1_score.append(self.avg_f1_score)\n",
    "        \n",
    "    \n",
    "    def save(self, epoch, file_path=\"../models/2k/\"):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        output_path = file_path + f\"crnn_ep{epoch}.model\"\n",
    "        torch.save(\n",
    "            {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.cpu().state_dict(),\n",
    "            'optimizer_state_dict': self.optim.state_dict(),\n",
    "            'criterion': self.criterion\n",
    "            },\n",
    "            output_path)\n",
    "        self.model.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "    \n",
    "    def export_log(self, epoch, file_path=\"../../logs/2k/\"):\n",
    "        df = pd.DataFrame({\n",
    "            \"train_loss\": self.train_loss, \n",
    "            \"test_loss\": self.test_loss, \n",
    "            \"train_F1_score\": self.train_f1_score,\n",
    "            \"test_F1_score\": self.test_f1_score\n",
    "        })\n",
    "        output_path = file_path+f\"loss_timestrech.log\"\n",
    "        print(\"EP:%d logs Saved on:\" % epoch, output_path)\n",
    "        df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use: cuda:0\n",
      "Using 1 GPUS for Converter\n",
      "Total Parameters: 26176777\n"
     ]
    }
   ],
   "source": [
    "folder = \"../../dataset/tensor_audio\"\n",
    "\n",
    "with open('../../dataset/train_data.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "with open('../../dataset/test_data.pickle', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "            \n",
    "\n",
    "train_dataset = LoadTrainDataset(train_data, folder)\n",
    "test_dataset = LoadTrainDataset(test_data, folder)\n",
    "\n",
    "batch_size = 32\n",
    "num_workers= 5\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "lr = 1e-3\n",
    "weight_decay=0.0\n",
    "adam_beta1=0.9\n",
    "adam_beta2=0.999\n",
    "betas = (adam_beta1, adam_beta2)\n",
    "\n",
    "log_freq=100\n",
    "with_cuda=True\n",
    "\n",
    "model = None\n",
    "\n",
    "trainer = Trainer(train_dataloader, test_dataloader, lr, betas, weight_decay, log_freq, with_cuda, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 535/535 [01:29<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 534, 'avg_loss': 0.21258, 'loss': 0.02792, 'avg_f1_score': 0.00473}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:0: 100%|| 134/134 [00:12<00:00, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 133, 'avg_loss': 0.02819, 'loss': 0.02789, 'avg_f1_score': 0.00513}\n",
      "EP:0 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:1: 100%|| 535/535 [01:29<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 534, 'avg_loss': 0.0257, 'loss': 0.02481, 'avg_f1_score': 0.00648}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:1: 100%|| 134/134 [00:12<00:00, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 133, 'avg_loss': 0.02483, 'loss': 0.02484, 'avg_f1_score': 0.01096}\n",
      "EP:1 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:2: 100%|| 535/535 [01:29<00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 534, 'avg_loss': 0.02473, 'loss': 0.02434, 'avg_f1_score': 0.0077}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:2: 100%|| 134/134 [00:12<00:00, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 133, 'avg_loss': 0.02459, 'loss': 0.02446, 'avg_f1_score': 0.01143}\n",
      "EP:2 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:3: 100%|| 535/535 [01:29<00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 534, 'avg_loss': 0.02442, 'loss': 0.02322, 'avg_f1_score': 0.01174}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:3: 100%|| 134/134 [00:12<00:00, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 133, 'avg_loss': 0.02461, 'loss': 0.02472, 'avg_f1_score': 0.01353}\n",
      "EP:3 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:4: 100%|| 535/535 [01:29<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 534, 'avg_loss': 0.02398, 'loss': 0.02254, 'avg_f1_score': 0.01647}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:4: 100%|| 134/134 [00:12<00:00, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 133, 'avg_loss': 0.02393, 'loss': 0.0243, 'avg_f1_score': 0.01835}\n",
      "EP:4 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:5: 100%|| 535/535 [01:29<00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 534, 'avg_loss': 0.02366, 'loss': 0.0214, 'avg_f1_score': 0.02202}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:5: 100%|| 134/134 [00:12<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 133, 'avg_loss': 0.02312, 'loss': 0.02378, 'avg_f1_score': 0.03032}\n",
      "EP:5 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:6: 100%|| 535/535 [01:29<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 534, 'avg_loss': 0.02325, 'loss': 0.02375, 'avg_f1_score': 0.02996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:6: 100%|| 134/134 [00:12<00:00, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 133, 'avg_loss': 0.02276, 'loss': 0.02241, 'avg_f1_score': 0.03887}\n",
      "EP:6 logs Saved on: ../../logs/2k/loss_timestrech.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:7:   5%|| 26/535 [00:04<01:23,  6.10it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ab035a27fd3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Model Save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-29b25eaf99e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-29b25eaf99e7>\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mloss_store\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mf1_score_store\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredict_f1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_store\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "print(\"Training Start\")\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    trainer.train(epoch)\n",
    "    # Model Save\n",
    "    trainer.test(epoch)\n",
    "    trainer.export_log(epoch)\n",
    "    if epoch % 50 == 0 and epoch != 0:\n",
    "        trainer.save(epoch)\n",
    "trainer.save(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
